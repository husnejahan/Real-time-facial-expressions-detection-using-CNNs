{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import gzip\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from numpy import genfromtxt\n",
    "from lasagne.layers import batch_norm ,dropout,DenseLayer\n",
    "\n",
    "conv = lasagne.layers.Conv2DLayer\n",
    "pool = lasagne.layers.MaxPool2DLayer\n",
    "NUM_EPOCHS = 500\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "DIM = 48\n",
    "DATA_SIZE = 35887\n",
    "NUM_CLASSES = 10\n",
    "FILE_NAME = \"fer2013/fer2013.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: (28709, 1, 48, 48)\n",
      "Validation samples: (3589, 1, 48, 48)\n",
      "Test samples: (3589, 1, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    X_data = np.zeros((DATA_SIZE,2304))\n",
    "    Y_data = np.zeros((DATA_SIZE,1))\n",
    "    with open(FILE_NAME, 'rt') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        i=0\n",
    "        for row in reader:\n",
    "            X_data[i, :] = np.fromstring(row['pixels'], dtype=int, sep=' ')\n",
    "            Y_data[i] = row['emotion']\n",
    "            i = i + 1\n",
    "    \n",
    "    \n",
    "\n",
    "    num_all = X_data.shape[0]\n",
    "    mask = np.random.choice(num_all, int(num_all))\n",
    "    \n",
    "    \n",
    "    X_train, X_valid, X_test = np.split(X_data[mask,:], [int(.8*len(X_data)), int(.9*len(X_data))])\n",
    "    y_train, y_valid, y_test = np.split(Y_data[mask,0], [int(.8*len(Y_data)), int(.9*len(Y_data))])\n",
    "    y_train = y_train.astype('int32')\n",
    "    y_valid = y_valid.astype('int32')\n",
    "    y_test = y_test.astype('int32')\n",
    "    \n",
    "    # reshape for convolutions\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, DIM, DIM))\n",
    "    X_valid = X_valid.reshape((X_valid.shape[0], 1, DIM, DIM))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, DIM, DIM))\n",
    "    \n",
    "    print (\"Train samples:\", X_train.shape)\n",
    "    print (\"Validation samples:\", X_valid.shape)\n",
    "    print (\"Test samples:\", X_test.shape)\n",
    "\n",
    "    return dict(\n",
    "        X_train=lasagne.utils.floatX(X_train),\n",
    "        y_train=y_train.astype('int32'),\n",
    "        X_valid=lasagne.utils.floatX(X_valid),\n",
    "        y_valid=y_valid.astype('int32'),\n",
    "        X_test=lasagne.utils.floatX(X_test),\n",
    "        y_test=y_test.astype('int32'),\n",
    "        num_examples_train=X_train.shape[0],\n",
    "        num_examples_valid=X_valid.shape[0],\n",
    "        num_examples_test=X_test.shape[0],\n",
    "        input_height=X_train.shape[2],\n",
    "        input_width=X_train.shape[3],\n",
    "        output_dim=7,)\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_width, input_height, output_dim):\n",
    "    ini = lasagne.init.HeUniform(gain='relu')\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 1, input_width, input_height),)\n",
    "    \n",
    "    \n",
    "    #class_l5 = pool(class_l4, pool_size=(2, 2))\n",
    "    #class_b1 = lasagne.layers.batch_norm(class_l1) #batchnorm\n",
    "    #class_d1 = lasagne.layers.DropoutLayer(class_l2) #  dropout\n",
    "    \n",
    "    # Classi32fication network\n",
    "    #stack_1 = batch_norm(ConvLayer(l, num_filters=out_num_filters,filter_size=(3,3), stride=first_stride, nonlinearity=rectify, pad='same', W=lasagne.init.HeNormal(gain='relu'), flip_filters=False))\n",
    "    class_l1 = batch_norm(conv(\n",
    "        l_in,\n",
    "        num_filters=32,pad='same',\n",
    "        filter_size=(3, 3),\n",
    "        #filter_size=(5, 5),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=ini,\n",
    "    ))\n",
    "    class_d0 = dropout(class_l1,p=0.3)\n",
    "    \n",
    "    class_l2 = batch_norm(conv(\n",
    "        class_d0,\n",
    "        num_filters=32,pad='same',\n",
    "        filter_size=(3, 3),\n",
    "        #filter_size=(5, 5),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=ini,\n",
    "    ))\n",
    "    class_d1 = dropout(class_l2,p=0.3)\n",
    "    \n",
    "    \n",
    "    class_l3 = batch_norm(conv(\n",
    "        class_d1,\n",
    "        num_filters=32,pad='same',\n",
    "        filter_size=(3, 3),\n",
    "        #filter_size=(5, 5),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=ini,\n",
    "    ))\n",
    "    class_d2 = dropout(class_l3,p=0.35)\n",
    "    \n",
    "    \n",
    "    class_l4 = batch_norm(conv(\n",
    "        class_d2,\n",
    "        num_filters=32,pad='same',\n",
    "        filter_size=(3, 3),\n",
    "        #filter_size=(5, 5),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=ini,\n",
    "    ))\n",
    "    class_d3 = dropout(class_l4,p=0.2)   \n",
    "    #class_l5 = pool(class_l4, pool_size=(2, 2))\n",
    "    \n",
    "    \n",
    "    class_l1_dens = batch_norm(DenseLayer(\n",
    "        class_d3,\n",
    "        num_units=32,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=ini,\n",
    "    )) \n",
    "    class_d1_dens = dropout(class_l1_dens,0.3)\n",
    "    \n",
    "    class_l2_dens = batch_norm(DenseLayer(\n",
    "        class_d1_dens,\n",
    "        num_units=32,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=ini,\n",
    "    )) \n",
    "    class_d2_dens = dropout(class_l2_dens,p=0.2)\n",
    "    \n",
    "    l_out = DenseLayer(\n",
    "        class_d2_dens,\n",
    "        num_units=output_dim,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax,\n",
    "        W=ini,\n",
    "    )\n",
    "    return l_out\n",
    "\n",
    "model= build_model(DIM, DIM, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the graph in theano\n",
    "sym_x = T.tensor4('sym_x') # a symbolic variable, this is now a 4-D tensor.\n",
    "sym_t = T.ivector('sym_t') # a symbolic variable taking on the value of the target batch.\n",
    "\n",
    "# Retrieve network output\n",
    "train_out = lasagne.layers.get_output(model, sym_x, deterministic=False)\n",
    "eval_out = lasagne.layers.get_output(model, sym_x, deterministic=True)\n",
    "\n",
    "# Retrieve list of all trainable parameters in the network.\n",
    "all_params = lasagne.layers.get_all_params(model, trainable=True)\n",
    "\n",
    "# add weight decay\n",
    "all_layers = lasagne.layers.get_all_layers(model)\n",
    "l2_penalty = lasagne.regularization.regularize_layer_params(all_layers, lasagne.regularization.l2) * 0.001\n",
    "\n",
    "\n",
    "#reg2 = lasagne.regularization.l2(train_out)\n",
    "#reg = lasagne.regularization.l1( train_out )\n",
    "cost = lasagne.objectives.categorical_crossentropy(train_out+1e-8, sym_t).mean()\n",
    "\n",
    "# Let Theano do its magic and get all the gradients we need for training\n",
    "all_grads = T.grad(cost, all_params)\n",
    " \n",
    "# Set the update function for parameters \n",
    "# you might wan't to experiment with more advanded update schemes like rmsprob, adadelta etc.\n",
    "sh_lr = theano.shared(lasagne.utils.floatX(LEARNING_RATE))\n",
    "\n",
    "Updates = lasagne.updates.adam(all_grads, all_params, learning_rate=sh_lr)\n",
    "\n",
    "f_eval = theano.function([sym_x],eval_out, on_unused_input='warn')\n",
    "\n",
    "f_train = theano.function([sym_x, sym_t],[cost],updates=Updates, on_unused_input='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "NUM_CLASSES = 7\n",
    "from confusionmatrix import ConfusionMatrix\n",
    "batch_size = 100\n",
    "num_epochs = 20\n",
    "num_samples_train = data['X_train'].shape[0]                    #data['X_train'] data['X_valid'] data['X_test']\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = data['X_valid'].shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "num_samples_test = data['X_test'].shape[0]\n",
    "num_batches_test = num_samples_test // batch_size\n",
    "\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "loss = []\n",
    "#with np.load('para_d2.npz') as f:\n",
    "#     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "#lasagne.layers.set_all_param_values( model, param_values)\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        #Forward->Backprob->Update params\n",
    "        confusion_valid = ConfusionMatrix(NUM_CLASSES)\n",
    "        confusion_train = ConfusionMatrix(NUM_CLASSES)\n",
    "        confusion_test = ConfusionMatrix(NUM_CLASSES)\n",
    "        \n",
    "        \n",
    "        cur_loss = 0\n",
    "        for i in range(num_batches_train):\n",
    "            idx = range(i*batch_size, (i+1)*batch_size)\n",
    "            x_batch = data['X_train'][idx]\n",
    "            target_batch = data['y_train'][idx]    \n",
    "            batch_loss = f_train(x_batch,target_batch) #this will do the complete backprob pass\n",
    "            cur_loss += batch_loss[0]\n",
    "        loss += [cur_loss/batch_size]\n",
    "        \n",
    "        for i in range(num_batches_train):\n",
    "            idx = range(i*batch_size, (i+1)*batch_size)\n",
    "            x_batch = data['X_train'][idx]\n",
    "            targets_batch = data['y_train'][idx]    \n",
    "            net_out = f_eval(x_batch)   \n",
    "            preds = np.argmax(net_out, axis=-1) \n",
    "            confusion_train.batch_add(targets_batch, preds)\n",
    "\n",
    "        for i in range(num_batches_valid):\n",
    "            idx = range(i*batch_size, (i+1)*batch_size)\n",
    "            x_batch = data['X_valid'][idx]\n",
    "            targets_batch = data['y_valid'][idx]\n",
    "            net_out = f_eval(x_batch)   \n",
    "            preds = np.argmax(net_out, axis=-1) \n",
    "            confusion_valid.batch_add(targets_batch, preds)\n",
    "            \n",
    "        for i in range(num_batches_test):\n",
    "            idx = range(i*batch_size, (i+1)*batch_size)\n",
    "            x_batch = data['X_test'][idx]\n",
    "            targets_batch = data['y_test'][idx]\n",
    "            net_out = f_eval(x_batch)   \n",
    "            preds = np.argmax(net_out, axis=-1)\n",
    "            confusion_test.batch_add(targets_batch, preds)\n",
    "            \n",
    "                \n",
    "        \n",
    "        train_acc_cur = confusion_train.accuracy()\n",
    "        valid_acc_cur = confusion_valid.accuracy()\n",
    "        test_acc_cur = confusion_test.accuracy()\n",
    "        \n",
    "        train_acc += [train_acc_cur]\n",
    "        valid_acc += [valid_acc_cur]\n",
    "        test_acc += [test_acc_cur]\n",
    "        np.savez('para.npz', *lasagne.layers.get_all_param_values(model))\n",
    "        if (i+1) % 10 == 0:\n",
    "            new_lr = sh_lr.get_value() * 0.7\n",
    "            sh_lr.set_value(lasagne.utils.floatX(new_lr))\n",
    "        print(\"Epoch %i : Train Loss %e , Train acc %f,  Valid acc %f ,Test acc %f , acc : %f\"\\\n",
    "        % (epoch+1, loss[-1], train_acc_cur*100, valid_acc_cur*100,test_acc_cur*100,\n",
    "           max(train_acc_cur-valid_acc_cur,train_acc_cur-test_acc_cur)*100))\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "    \n",
    "\n",
    "#get test set score\n",
    "confusion_test = ConfusionMatrix(NUM_CLASSES)\n",
    "net_out = f_eval(x_test)    \n",
    "preds = np.argmax(net_out, axis=-1)\n",
    "for i in range(3):\n",
    "    plt.subplot(321+i*2)\n",
    "confusion_test.batch_add(targets_test, preds)\n",
    "print(\"\\nTest set Acc:  %f\" %(confusion_test.accuracy()))\n",
    "\n",
    "\n",
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure()\n",
    "plt.plot(epoch,train_acc,'r',epoch,valid_acc,'b')\n",
    "plt.legend(['Train Acc','Val Acc'])\n",
    "plt.xlabel('Epochs'), plt.ylabel('Acc'), plt.ylim([0.75,1.03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
